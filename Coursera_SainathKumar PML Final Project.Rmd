---
title: "PML Sainath Kumar"
author: "Sai"
date: "July 29, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
 

 
```{r cars} 
library(caret)
library(ISLR)
library(ggplot2)
library(Hmisc)
library(splines)
library(rpart) 
library(ElemStatLearn) 


```
Load the training and testing data sets


```{r}
setwd("C:/Users/IBM_ADMIN/Documents/Coursera/Practical Machine LEarning")
train <-read.csv("pml-training.csv",header=TRUE)
test <-read.csv("pml-testing.csv",header=TRUE)
dim(train)
```


Analyze the data set

Based on initial look at the data the data has blanks , NA ,#DIV/0
First step would be to cleanse the data
Also at first glance the first 7 columns do not seem like the ones that would have effect on the prediction outcome . 

 
```{r}
train <-read.csv("pml-training.csv",header=TRUE,na.strings=c("NA","#DIV/0!",""))
test <-read.csv("pml-testing.csv",header=TRUE,na.strings=c("NA","#DIV/0!",""))
train <- train[,8:160]
```

 
<h1>Identifying and removing near zero variance variables</h1>

 
```{r}
train_var <- nearZeroVar(train, saveMetrics=TRUE)
train_var1 <- train[,train_var$nzv==FALSE]

train_1 <- train_var1
for(i in 1:length(train_var1)) {
  if( sum( is.na( train_var1[, i] ) ) /nrow(train_var1) >= .6) {
    for(j in 1:length(train_1)) {
      if( length( grep(names(train_var1[i]), names(train_1)[j]) ) == 1)  {
        train_1 <- train_1[ , -j]
      }
    }
  }
}
train_var1 <- train_1
rm(train_1)
dim(train_var1)
```

 
#Applying the same transformations to the testing set as well
#Since here we have removed the near zero variables and NA variables testing data should also have only these variables</h3>

 
```{r}
train_variables <- colnames(train_var1)
train_variables_1 <- colnames(train_var1[, -53])  # remove the classe column
test <- test[train_variables_1]
dim(test)
```


<h1>Partitioning the training data</h1>
 
```{r}
intrain <- createDataPartition(train_var1$classe, p=0.7, list=FALSE)
train_fin <- train_var1[intrain, ]
valid_fin <- train_var1[-intrain, ]
dim(train_fin)
dim(valid_fin)
```

 


<h1>Using basic classification tree algorithm to predict the classe</h1>
 
```{r}
modfit <- train(train_fin$classe ~ ., data = train_fin, method="rpart")
print(modfit)
predict_1 <- predict(modfit, newdata=valid_fin)
print(confusionMatrix(predict_1,valid_fin$classe))
```
 
<h3>the accuracy of the prediction deteriorated further less that 50% therefore this would not be a good model to predict
Do a check if preprocessing the data will improve the prediction results</h3>
 
```{r}
modfit_1 <- train(train_fin$classe ~ ., data = train_fin, method="rpart", preProcess=c("center", "scale"))
print(modfit_1)
predict_1 <- predict(modfit_1, newdata=valid_fin)
print(confusionMatrix(predict_1,valid_fin$classe))
```
 
<h4>Preprocessing does not improve the accruacy further therfore proceeding to use other algrithms to build the model</h4>
<h1>Random forests  to be used to build further models</h1>


 
#Lines Commented for Kniting as RF take long time to process. Please see Original R code for details
```{r}



# modfit_2 <- train(train_fin$classe ~ ., data = train_fin, method="rf", preProcess=c("center", "scale"))
# print(modfit_2)
# predict_2 <- predict(modfit_2, newdata=valid_fin)
# print(confusionMatrix(predict_2,valid_fin$classe))

# > modfit_2 <- train(train_fin$classe ~ ., data = train_fin, method="rf", preProcess=c("center", "scale"))
# >
#   > print(modfit_2)
# Random Forest
#
# 13737 samples
# 52 predictor
# 5 classes: 'A', 'B', 'C', 'D', 'E'
#
# Pre-processing: centered (52), scaled (52)
# Resampling: Bootstrapped (25 reps)
# Summary of sample sizes: 13737, 13737, 13737, 13737, 13737, 13737, ...
# Resampling results across tuning parameters:
#
#   mtry  Accuracy   Kappa
# 2    0.9881682  0.9850310
# 27    0.9887566  0.9857768
# 52    0.9811102  0.9761049
#
# Accuracy was used to select the optimal model using  the largest value.
# The final value used for the model was mtry = 27.
# >
#   > #Running the Model against the test class
#   >
#   > predict_2 <- predict(modfit_2, newdata=valid_fin)
# > print(confusionMatrix(predict_2,valid_fin$classe))
# Confusion Matrix and Statistics
#
# Reference
# Prediction    A    B    C    D    E
# A 1674   15    0    0    0
# B    0 1123    4    0    0
# C    0    1 1017    9    0
# D    0    0    5  955    0
# E    0    0    0    0 1082
#
# Overall Statistics
#
# Accuracy : 0.9942
# 95% CI : (0.9919, 0.996)
# No Information Rate : 0.2845
# P-Value [Acc > NIR] : < 2.2e-16
#
# Kappa : 0.9927
# Mcnemar's Test P-Value : NA
#
# Statistics by Class:
#
# Class: A Class: B Class: C Class: D Class: E
# Sensitivity            1.0000   0.9860   0.9912   0.9907   1.0000
# Specificity            0.9964   0.9992   0.9979   0.9990   1.0000
# Pos Pred Value         0.9911   0.9965   0.9903   0.9948   1.0000
# Neg Pred Value         1.0000   0.9966   0.9981   0.9982   1.0000
# Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
# Detection Rate         0.2845   0.1908   0.1728   0.1623   0.1839
# Detection Prevalence   0.2870   0.1915   0.1745   0.1631   0.1839
# Balanced Accuracy      0.9982   0.9926   0.9946   0.9948   1.0000


#print(plot(varImp(modfit_2, scale = FALSE)))

#Applying the model to the final test data for 20 cases
# Run against 20 testing set provided by Professor Leek.
#print(predict(modfit_2, newdata=test))
# [1] B A B A A E D B A A B C B A E E A B B B
# Levels: A B C D E

#FINAL OUTPUT FOR TEST CASE

# Results <- data.frame(predicted=predict(modfit_2, newdata=test)
# )
#    predicted
# 1          B
# 2          A
# 3          B
# 4          A
# 5          A
# 6          E
# 7          D
# 8          B
# 9          A
# 10         A
# 11         B
# 12         C
# 13         B
# 14         A
# 15         E
# 16         E
# 17         A
# 18         B
# 19         B
# 20         B
 
```

<h1>Conclusion:</h1>
</h3>
Based on the RF model the test cases have been predicted as above.The accuracy for the RF model is relatively higher and would certainly provide more accurate results
Additional Analysis and further explanation:
I would like to have done additional exploratory data analysis to reduce the number of predictors down further. PCA- Principal component analysis as well as using concepts from Regularized regression and combining predictor chapter.</h3>


```{r pressure, echo=FALSE}
 
```
 
